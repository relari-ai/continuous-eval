---
title: Introduction
description: Overview
sidebar:
  badge:
    text: new
    variant: tip
---

## What is continuous-eval?

`continuous-eval` is an open-source package created for the scientific and practical evaluation of LLM application pipelines. Currently, it focuses on retrieval-augmented generation (RAG) pipelines.

## Why run evaluation?

Running evaluation can help you 1) identify pipeline strengths and weaknesses, 2) know what / how to improve, 3) accelerate your development.

## Why use continuous eval?

- **Most Complete RAG Metric Library:** mix and match Deterministic, Semantic and LLM-based metrics.

- **Trustworthy Ensemble Metric:** easily build a close-to-human ensemble evaluation pipeline with mathematical guarantees.

- **Cheaper and Faster:** our hybrid pipeline slashes cost by up to 15x compared to pure LLM-based metrics, and reduces eval time on large datasets from hours to minutes.

- **Tailored to Your Data:** our evaluation pipeline can be customized to your use case and leverages the data you trust. We can help you curate a golden dataset if you donâ€™t have one.


## Resources

- **A Practical Guide to RAG Pipeline Evaluation:** [Part 1: Retrieval](https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893), [Part 2: Generation](https://medium.com/relari/a-practical-guide-to-rag-evaluation-part-2-generation-c79b1bde0f5d)
- Discord: Join our community of LLM developers [Discord](https://discord.gg/GJnM8SRsHr).
- Talk to founders: [email](founders@relari.ai)