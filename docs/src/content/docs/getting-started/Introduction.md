---
title: Introduction
description: Overview
sidebar:
  badge:
    text: new
    variant: tip
---

## What is continuous-eval?

`continuous-eval` is an open-source package created for granular and holistic evaluation of GenAI application pipelines.

<img src="/v0.3/module-level-eval.png"></img>

## How is continuous-eval different?

- **Modularized Evaluation**: Measure each module in the pipeline with tailored metrics.

- **Comprehensive Metric Library**: Covers Retrieval-Augmented Generation (RAG), Code Generation, Agent Tool Use, Classification and a variety of other LLM use cases. Mix and match Deterministic, Semantic and LLM-based metrics.

## Resources

- **Relari Blog:** Useful articles on how to evaluate LLM applications [link](https://www.relari.ai/blog)
- **Discord:** Join our community of LLM developers [Discord](https://discord.gg/GJnM8SRsHr)
- **Reach out to founders:** [Email](mailto:founders@relari.ai) or [Schedule a chat](https://cal.com/pasquale/continuous-eval)
