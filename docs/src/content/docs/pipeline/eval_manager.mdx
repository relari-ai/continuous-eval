---
title: Eval Manager
sidebar:
  badge:
    text: new
    variant: tip
---


## Definition
`eval_manager` class managers

Leveraging the pipeline defined in


## Example

### Logging data in evaluation runs

```python title="example_app.py"
import os
from pathlib import Path
from langchain.retrievers.document_compressors import CohereRerank
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv
from continuous_eval.eval.manager import eval_manager
from examples.langchain.simple_rag.pipeline import pipeline
load_dotenv()

def retrieve(q):
    db = Chroma(
        persist_directory=str("/Users/yisz/Downloads/208_219_chroma_db"),
        embedding_function=OpenAIEmbeddings(),
    )
    retriever = db.as_retriever(
        search_type="similarity", search_kwargs={"k": 10}
    )
    return retriever.invoke(q)

def rerank(q, retrieved_docs):
    compressor = CohereRerank(cohere_api_key=os.getenv("COHERE_API_KEY"))
    return compressor.compress_documents(retrieved_docs, q)

def ask(q, retrieved_docs):
    model = ChatGoogleGenerativeAI(model="gemini-pro")
    system_prompt = (
        "You are and expert of the life of Paul Graham.\n"
        "Answer the question below based on the context provided."
    )
    user_prompt = f"Question: {q}\n\n"
    user_prompt += "Contexts:\n" + "\n".join(
        [doc.page_content for doc in retrieved_docs]
    )
    try:
        result = model.invoke(system_prompt + user_prompt).content
    except Exception as e:
        print(e)
        result = "Sorry, I cannot answer this question."
    return result

if __name__ == "__main__":
    eval_manager.set_pipeline(pipeline)
    eval_manager.start_run()
    while eval_manager.is_running():
        if eval_manager.curr_sample is None:
            break
        q = eval_manager.curr_sample["question"]
        # Run and log Retriever results
        retrieved_docs = retrieve(q)
        eval_manager.log("retriever", [doc.__dict__ for doc in retrieved_docs])
        # Run and log Reranker results
        reranked_docs = rerank(q, retrieved_docs)
        eval_manager.log("reranker", [doc.__dict__ for doc in reranked_docs])
        # Run and log Generator results
        response = ask(q, retrieved_docs)
        eval_manager.log("llm", response)
        print(f"Q: {q}\nA: {response}\n")
        eval_manager.next_sample()

    eval_manager.evaluation.save(Path("results.jsonl"))
```

Peek at one sample in `results.jsonl`:
```json
{
  "retriever": [
    {
      "page_content": "The reason this is mistaken is that people do sometimes change what they want. People who don't want to want something -- drug addicts, for example -- can sometimes make themselves stop wanting it. And people who want to want something -- who want to like classical music, or broccoli -- sometimes succeed.\n\nSo we modify our initial statement: You can do what you want, but you can't want to want what you want.",
      "metadata": {
        "source": "../data/graham_essays/215_what_you_want_to_want.txt",
        "relevance_score": 0.49873924
      },
      "type": "Document"
    },
    ... (9 chunks)
  ],
  "reranker": [
    {
      "page_content": "That's still not quite true. It's possible to change what you want to want. I can imagine someone saying \"I decided to stop wanting to like classical music.\" But we're getting closer to the truth. It's rare for people to change what they want to want, and the more \"want to\"s we add, the rarer it gets.",
      "metadata": {
        "source": "../data/graham_essays/215_what_you_want_to_want.txt",
        "relevance_score": 0.66115755
      },
      "type": "Document"
    },
    ... (3 chunks)
  ],
  "llm": "Yes, people can change their desires.\n\nThe passage states that \"people do sometimes change what they want\" and that \"people who want to want something -- who want to like classical music, or broccoli -- sometimes succeed.\" It also acknowledges that \"it's possible to change what you want to want,\" though it notes that this is rare."
}
```


### Run evaluators and tests on results

```python title="eval.py"
from pathlib import Path

from continuous_eval.eval.manager import eval_manager
from examples.langchain.simple_rag.pipeline import pipeline

if __name__ == "__main__":
    eval_manager.set_pipeline(pipeline)

    # #Evaluation
    # eval_manager.evaluation.load(Path("results.jsonl"))
    # eval_manager.run_metrics()
    # eval_manager.metrics.save(Path("metrics_results.json"))

    # Tests
    eval_manager.metrics.load(Path("metrics_results.json"))
    agg = eval_manager.metrics.aggregate()
    print(agg)
    eval_manager.run_tests()
    eval_manager.tests.save(Path("test_results.json"))

    # eval_manager.tests.load(Path("test_results.json"))
    for module_name, test_results in eval_manager.tests.results.items():
        print(f"{module_name}")
        for test_name in test_results:
            print(f" - {test_name}: {test_results[test_name]}")
    print("Done")
```

PRINT results
